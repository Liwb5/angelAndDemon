{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liwb/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/liwb/anaconda3/lib/python3.5/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import cross_validation, metrics   #Additional     scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt  \n",
    "import sys\n",
    "import os\n",
    "from sklearn.externals import joblib #joblib模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112405, 1127) (112405,) (28101, 1127)\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/home/liwb/Documents/projects/angelAndDemon/')\n",
    "version='10'\n",
    "trainPath = './dataAfterProcess/trainRes%s.csv'%(version)\n",
    "trainData = pd.read_csv(trainPath, header=None)\n",
    "X = trainData.values[:,0:trainData.shape[1]-1]\n",
    "y = trainData.values[:,trainData.shape[1]-1]\n",
    "\n",
    "testPath = './dataAfterProcess/testRes%s.csv'%(version)\n",
    "testData = pd.read_csv(testPath, header=None)\n",
    "testX = testData.values\n",
    "print(X.shape, y.shape, testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112405, 1135) (112405,) (28101, 1135)\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/home/liwb/Documents/projects/angelAndDemon/')\n",
    "version='0506jp'\n",
    "trainPath = './code/dataAfterProcess/trainResult.csv'\n",
    "trainData = pd.read_csv(trainPath, header=None)\n",
    "X5 = trainData.values[:,0:trainData.shape[1]-1]\n",
    "y5 = trainData.values[:,trainData.shape[1]-1]\n",
    "\n",
    "testPath = './code/dataAfterProcess/testResult.csv'\n",
    "testData = pd.read_csv(testPath, header=None)\n",
    "testX5 = testData.values\n",
    "print(X5.shape, y5.shape, testX5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1127</th>\n",
       "      <th>1128</th>\n",
       "      <th>1129</th>\n",
       "      <th>1130</th>\n",
       "      <th>1131</th>\n",
       "      <th>1132</th>\n",
       "      <th>1133</th>\n",
       "      <th>1134</th>\n",
       "      <th>1135</th>\n",
       "      <th>1136</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1190.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>415.97938</td>\n",
       "      <td>620.968964</td>\n",
       "      <td>877.680382</td>\n",
       "      <td>1146.383491</td>\n",
       "      <td>1146.383491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>1210.0</td>\n",
       "      <td>194.91286</td>\n",
       "      <td>494.383970</td>\n",
       "      <td>920.330460</td>\n",
       "      <td>1135.390750</td>\n",
       "      <td>1135.390750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1190.0</td>\n",
       "      <td>1190.0</td>\n",
       "      <td>43.90458</td>\n",
       "      <td>353.937430</td>\n",
       "      <td>740.716240</td>\n",
       "      <td>1018.044300</td>\n",
       "      <td>1018.044300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>539.90534</td>\n",
       "      <td>705.979770</td>\n",
       "      <td>951.631810</td>\n",
       "      <td>1122.279140</td>\n",
       "      <td>1122.279140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1090.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>178.67294</td>\n",
       "      <td>408.822770</td>\n",
       "      <td>884.255890</td>\n",
       "      <td>1202.088670</td>\n",
       "      <td>1202.088670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>1120.0</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>1120.0</td>\n",
       "      <td>598.35052</td>\n",
       "      <td>768.618360</td>\n",
       "      <td>1052.501220</td>\n",
       "      <td>1131.065150</td>\n",
       "      <td>1131.065150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>163.47104</td>\n",
       "      <td>446.478960</td>\n",
       "      <td>847.010450</td>\n",
       "      <td>1183.178120</td>\n",
       "      <td>1183.178120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>838.02254</td>\n",
       "      <td>914.214260</td>\n",
       "      <td>1082.617210</td>\n",
       "      <td>1130.764800</td>\n",
       "      <td>1130.764800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>801.81674</td>\n",
       "      <td>884.506120</td>\n",
       "      <td>1141.161640</td>\n",
       "      <td>1226.581250</td>\n",
       "      <td>1226.581250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1160.0</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>149.14342</td>\n",
       "      <td>433.006390</td>\n",
       "      <td>910.774860</td>\n",
       "      <td>1177.367040</td>\n",
       "      <td>1177.367040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1137 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0       1       2       3       4          5           6     \\\n",
       "10  1.290000e+11  1190.0  1140.0  1230.0  1230.0  415.97938  620.968964   \n",
       "11  1.290000e+11  1150.0  1040.0  1230.0  1210.0  194.91286  494.383970   \n",
       "12  1.290000e+11  1040.0  1100.0  1190.0  1190.0   43.90458  353.937430   \n",
       "13  1.280000e+11  1140.0  1100.0  1200.0  1200.0  539.90534  705.979770   \n",
       "14  1.290000e+11  1090.0  1140.0  1240.0  1200.0  178.67294  408.822770   \n",
       "15  1.280000e+11  1080.0  1120.0  1150.0  1120.0  598.35052  768.618360   \n",
       "16  1.290000e+11  1130.0  1150.0  1280.0  1250.0  163.47104  446.478960   \n",
       "17  1.280000e+11  1200.0  1070.0  1180.0  1170.0  838.02254  914.214260   \n",
       "18  1.280000e+11  1180.0  1180.0  1250.0  1200.0  801.81674  884.506120   \n",
       "19  1.280000e+11  1080.0  1070.0  1160.0  1130.0  149.14342  433.006390   \n",
       "\n",
       "           7            8            9     ...   1127  1128  1129  1130  1131  \\\n",
       "10   877.680382  1146.383491  1146.383491  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "11   920.330460  1135.390750  1135.390750  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "12   740.716240  1018.044300  1018.044300  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "13   951.631810  1122.279140  1122.279140  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "14   884.255890  1202.088670  1202.088670  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "15  1052.501220  1131.065150  1131.065150  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "16   847.010450  1183.178120  1183.178120  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "17  1082.617210  1130.764800  1130.764800  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "18  1141.161640  1226.581250  1226.581250  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "19   910.774860  1177.367040  1177.367040  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "    1132  1133  1134  1135  1136  \n",
       "10   0.0   0.0   0.0   0.0   0.0  \n",
       "11   0.0   0.0   0.0   0.0   1.0  \n",
       "12   0.0   0.0   0.0   0.0   1.0  \n",
       "13   0.0   0.0   0.0   0.0   0.0  \n",
       "14   0.0   0.0   0.0   0.0   1.0  \n",
       "15   0.0   0.0   0.0   0.0   0.0  \n",
       "16   0.0   0.0   0.0   0.0   1.0  \n",
       "17   0.0   0.0   0.0   0.0   0.0  \n",
       "18   0.0   0.0   0.0   0.0   0.0  \n",
       "19   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[10 rows x 1137 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X6 = pd.DataFrame(X5)\n",
    "X6[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112405, 1121)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X7 = X6.dropna(axis=1, thresh=10000)#删除nan很多的列\n",
    "X7.shape\n",
    "\n",
    "isReal = X6.applymap(np.isreal).all(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112405, 1137)\n",
      "330.0\n",
      "(28101, 1136)\n"
     ]
    }
   ],
   "source": [
    "#淼鑫的数据\n",
    "import pickle\n",
    "f = open('./dataAfterProcess/train_set.js','rb')\n",
    "train_set = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "train_set = np.array(train_set)\n",
    "\n",
    "f = open('./dataAfterProcess/valid_set.js','rb')\n",
    "valid_set = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "valid_set = np.array(valid_set)\n",
    "\n",
    "train_mx = np.vstack((train_set, valid_set))\n",
    "print(train_mx.shape)\n",
    "print(sum(train_mx[:,train_mx.shape[1]-1]))\n",
    "\n",
    "X_mx = train_mx[:,0:train_mx.shape[1]-1]\n",
    "y_mx = train_mx[:,train_mx.shape[1]-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f = open('./dataAfterProcess/test_set.js','rb')\n",
    "test_set = pickle.load(f)\n",
    "f.close()\n",
    "testX_mx = np.array(test_set)\n",
    "print(testX_mx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1126</th>\n",
       "      <th>1127</th>\n",
       "      <th>1128</th>\n",
       "      <th>1129</th>\n",
       "      <th>1130</th>\n",
       "      <th>1131</th>\n",
       "      <th>1132</th>\n",
       "      <th>1133</th>\n",
       "      <th>1134</th>\n",
       "      <th>1135</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>1090.0</td>\n",
       "      <td>1220.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>558.30968</td>\n",
       "      <td>708.226080</td>\n",
       "      <td>948.403690</td>\n",
       "      <td>1134.062410</td>\n",
       "      <td>1134.062410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1160.0</td>\n",
       "      <td>1090.0</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>612.52544</td>\n",
       "      <td>743.132960</td>\n",
       "      <td>978.090070</td>\n",
       "      <td>1155.537860</td>\n",
       "      <td>1155.537860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>1210.0</td>\n",
       "      <td>575.88264</td>\n",
       "      <td>752.691620</td>\n",
       "      <td>1063.356990</td>\n",
       "      <td>1175.861690</td>\n",
       "      <td>1175.861690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>1190.0</td>\n",
       "      <td>454.81484</td>\n",
       "      <td>636.541470</td>\n",
       "      <td>1012.651280</td>\n",
       "      <td>1170.493130</td>\n",
       "      <td>1170.493130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1090.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>1190.0</td>\n",
       "      <td>44.58538</td>\n",
       "      <td>352.586390</td>\n",
       "      <td>759.125630</td>\n",
       "      <td>1120.094080</td>\n",
       "      <td>1120.094080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1190.0</td>\n",
       "      <td>1190.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1210.0</td>\n",
       "      <td>661.38204</td>\n",
       "      <td>791.703530</td>\n",
       "      <td>1027.495210</td>\n",
       "      <td>1143.110060</td>\n",
       "      <td>1143.110060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>1220.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>700.17740</td>\n",
       "      <td>816.962350</td>\n",
       "      <td>1079.514640</td>\n",
       "      <td>1176.049290</td>\n",
       "      <td>1176.049290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>1090.0</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>1120.0</td>\n",
       "      <td>355.41880</td>\n",
       "      <td>622.715720</td>\n",
       "      <td>909.589260</td>\n",
       "      <td>1076.653750</td>\n",
       "      <td>1076.653750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1120.0</td>\n",
       "      <td>1030.0</td>\n",
       "      <td>1160.0</td>\n",
       "      <td>1090.0</td>\n",
       "      <td>34.70296</td>\n",
       "      <td>443.964570</td>\n",
       "      <td>920.295860</td>\n",
       "      <td>1105.681600</td>\n",
       "      <td>1105.681600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>232.14610</td>\n",
       "      <td>550.122420</td>\n",
       "      <td>900.886520</td>\n",
       "      <td>1145.012660</td>\n",
       "      <td>1145.012660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1190.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>165.97928</td>\n",
       "      <td>494.665340</td>\n",
       "      <td>892.622170</td>\n",
       "      <td>1146.710520</td>\n",
       "      <td>1146.710520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1060.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>27.76778</td>\n",
       "      <td>398.871630</td>\n",
       "      <td>878.638950</td>\n",
       "      <td>1151.256090</td>\n",
       "      <td>1151.256090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>997.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>1190.0</td>\n",
       "      <td>309.58848</td>\n",
       "      <td>534.714960</td>\n",
       "      <td>844.997670</td>\n",
       "      <td>1139.331930</td>\n",
       "      <td>1139.331930</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>591.59610</td>\n",
       "      <td>729.217430</td>\n",
       "      <td>1028.781450</td>\n",
       "      <td>1174.291830</td>\n",
       "      <td>1174.291830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>1210.0</td>\n",
       "      <td>239.61690</td>\n",
       "      <td>477.211190</td>\n",
       "      <td>1082.421980</td>\n",
       "      <td>1179.335950</td>\n",
       "      <td>1179.335950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>869.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>1220.0</td>\n",
       "      <td>169.32022</td>\n",
       "      <td>488.109160</td>\n",
       "      <td>895.711740</td>\n",
       "      <td>1171.815000</td>\n",
       "      <td>1171.815000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>1090.0</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>1220.0</td>\n",
       "      <td>62.59544</td>\n",
       "      <td>402.955530</td>\n",
       "      <td>879.205150</td>\n",
       "      <td>1155.545850</td>\n",
       "      <td>1155.545850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>448.80416</td>\n",
       "      <td>681.194930</td>\n",
       "      <td>895.755150</td>\n",
       "      <td>1144.835340</td>\n",
       "      <td>1144.835340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>767.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>612.49166</td>\n",
       "      <td>729.332930</td>\n",
       "      <td>983.504840</td>\n",
       "      <td>1119.321320</td>\n",
       "      <td>1119.321320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1120.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>719.94806</td>\n",
       "      <td>827.902570</td>\n",
       "      <td>1022.190300</td>\n",
       "      <td>1139.942710</td>\n",
       "      <td>1139.942710</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>363.48800</td>\n",
       "      <td>596.185070</td>\n",
       "      <td>952.453150</td>\n",
       "      <td>1148.269720</td>\n",
       "      <td>1148.269720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>1270.0</td>\n",
       "      <td>1190.0</td>\n",
       "      <td>173.35914</td>\n",
       "      <td>608.153880</td>\n",
       "      <td>851.943180</td>\n",
       "      <td>1140.837740</td>\n",
       "      <td>1140.837740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>599.86098</td>\n",
       "      <td>756.968120</td>\n",
       "      <td>930.289100</td>\n",
       "      <td>1095.713300</td>\n",
       "      <td>1095.713300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1160.0</td>\n",
       "      <td>1090.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1210.0</td>\n",
       "      <td>406.79626</td>\n",
       "      <td>655.088550</td>\n",
       "      <td>889.556670</td>\n",
       "      <td>1137.629790</td>\n",
       "      <td>1137.629790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>1120.0</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>188.10744</td>\n",
       "      <td>425.490250</td>\n",
       "      <td>786.081590</td>\n",
       "      <td>1058.253120</td>\n",
       "      <td>1058.253120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1160.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>212.65666</td>\n",
       "      <td>598.170500</td>\n",
       "      <td>982.697410</td>\n",
       "      <td>1134.660150</td>\n",
       "      <td>1134.660150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>1120.0</td>\n",
       "      <td>1160.0</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>599.07166</td>\n",
       "      <td>746.813160</td>\n",
       "      <td>971.101060</td>\n",
       "      <td>1137.097310</td>\n",
       "      <td>1137.097310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>1220.0</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>29.56814</td>\n",
       "      <td>544.755940</td>\n",
       "      <td>953.800690</td>\n",
       "      <td>1170.686600</td>\n",
       "      <td>1170.686600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>509.00020</td>\n",
       "      <td>676.688720</td>\n",
       "      <td>955.612560</td>\n",
       "      <td>1101.549150</td>\n",
       "      <td>1101.549150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>330.26994</td>\n",
       "      <td>497.478690</td>\n",
       "      <td>862.679560</td>\n",
       "      <td>1116.842050</td>\n",
       "      <td>1116.842050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>567.55728</td>\n",
       "      <td>789.215050</td>\n",
       "      <td>981.750750</td>\n",
       "      <td>1114.772670</td>\n",
       "      <td>1114.772670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>1190.0</td>\n",
       "      <td>48.61642</td>\n",
       "      <td>394.710580</td>\n",
       "      <td>819.896670</td>\n",
       "      <td>1113.943010</td>\n",
       "      <td>1113.943010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>1060.0</td>\n",
       "      <td>1190.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>35.30004</td>\n",
       "      <td>459.349300</td>\n",
       "      <td>872.120790</td>\n",
       "      <td>1101.754860</td>\n",
       "      <td>1101.754860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>1060.0</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>789.60168</td>\n",
       "      <td>843.854960</td>\n",
       "      <td>1014.573840</td>\n",
       "      <td>1123.998720</td>\n",
       "      <td>1123.998720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1060.0</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>486.79352</td>\n",
       "      <td>657.023690</td>\n",
       "      <td>970.936480</td>\n",
       "      <td>1153.051340</td>\n",
       "      <td>1153.051340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>1060.0</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>1120.0</td>\n",
       "      <td>514.34404</td>\n",
       "      <td>718.785770</td>\n",
       "      <td>1003.381780</td>\n",
       "      <td>1133.072490</td>\n",
       "      <td>1133.072490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1210.0</td>\n",
       "      <td>1120.0</td>\n",
       "      <td>1190.0</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>702.10014</td>\n",
       "      <td>841.772130</td>\n",
       "      <td>1014.189100</td>\n",
       "      <td>1160.711570</td>\n",
       "      <td>1160.711570</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>1190.0</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>302.25906</td>\n",
       "      <td>675.363880</td>\n",
       "      <td>972.452730</td>\n",
       "      <td>1142.146820</td>\n",
       "      <td>1142.146820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1120.0</td>\n",
       "      <td>1120.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>247.43284</td>\n",
       "      <td>563.351620</td>\n",
       "      <td>891.871790</td>\n",
       "      <td>1147.314940</td>\n",
       "      <td>1147.314940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1090.0</td>\n",
       "      <td>1090.0</td>\n",
       "      <td>1210.0</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>180.97344</td>\n",
       "      <td>457.798290</td>\n",
       "      <td>861.067280</td>\n",
       "      <td>1129.209610</td>\n",
       "      <td>1129.209610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1160.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>1190.0</td>\n",
       "      <td>614.03690</td>\n",
       "      <td>764.677650</td>\n",
       "      <td>1035.954150</td>\n",
       "      <td>1183.067300</td>\n",
       "      <td>1183.067300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>1030.0</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>610.63208</td>\n",
       "      <td>704.984400</td>\n",
       "      <td>955.040470</td>\n",
       "      <td>1130.543990</td>\n",
       "      <td>1130.543990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>49.60422</td>\n",
       "      <td>466.893070</td>\n",
       "      <td>836.263250</td>\n",
       "      <td>1123.286660</td>\n",
       "      <td>1123.286660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1160.0</td>\n",
       "      <td>1090.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>35.97522</td>\n",
       "      <td>306.738970</td>\n",
       "      <td>815.999450</td>\n",
       "      <td>1114.036320</td>\n",
       "      <td>1114.036320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1120.0</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>1220.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>673.04410</td>\n",
       "      <td>763.601830</td>\n",
       "      <td>1050.309650</td>\n",
       "      <td>1171.815510</td>\n",
       "      <td>1171.815510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>832.77608</td>\n",
       "      <td>907.869780</td>\n",
       "      <td>1076.301290</td>\n",
       "      <td>1144.761050</td>\n",
       "      <td>1144.761050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1090.0</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>215.15820</td>\n",
       "      <td>498.764150</td>\n",
       "      <td>925.245670</td>\n",
       "      <td>1158.808470</td>\n",
       "      <td>1158.808470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>896.0</td>\n",
       "      <td>1090.0</td>\n",
       "      <td>1190.0</td>\n",
       "      <td>1160.0</td>\n",
       "      <td>484.27250</td>\n",
       "      <td>696.529609</td>\n",
       "      <td>920.550427</td>\n",
       "      <td>1121.010545</td>\n",
       "      <td>1121.010545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1090.0</td>\n",
       "      <td>1030.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>710.46390</td>\n",
       "      <td>784.446420</td>\n",
       "      <td>958.820040</td>\n",
       "      <td>1092.210720</td>\n",
       "      <td>1092.210720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>1160.0</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>337.60326</td>\n",
       "      <td>578.371040</td>\n",
       "      <td>871.474070</td>\n",
       "      <td>1142.049530</td>\n",
       "      <td>1142.049530</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1120.0</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>47.20836</td>\n",
       "      <td>436.909080</td>\n",
       "      <td>848.331360</td>\n",
       "      <td>1109.410990</td>\n",
       "      <td>1109.410990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>1030.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>1210.0</td>\n",
       "      <td>60.51842</td>\n",
       "      <td>838.319240</td>\n",
       "      <td>1030.897330</td>\n",
       "      <td>1160.809520</td>\n",
       "      <td>1160.809520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>1010.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>569.71262</td>\n",
       "      <td>690.929560</td>\n",
       "      <td>932.362970</td>\n",
       "      <td>1076.660950</td>\n",
       "      <td>1076.660950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>862.0</td>\n",
       "      <td>1190.0</td>\n",
       "      <td>1190.0</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>157.31524</td>\n",
       "      <td>533.311710</td>\n",
       "      <td>917.982670</td>\n",
       "      <td>1176.793010</td>\n",
       "      <td>1176.793010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1120.0</td>\n",
       "      <td>1090.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1190.0</td>\n",
       "      <td>430.49524</td>\n",
       "      <td>630.382490</td>\n",
       "      <td>911.629650</td>\n",
       "      <td>1151.631660</td>\n",
       "      <td>1151.631660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1160.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>43.93032</td>\n",
       "      <td>316.842060</td>\n",
       "      <td>912.551180</td>\n",
       "      <td>1143.881630</td>\n",
       "      <td>1143.881630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>1030.0</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>1120.0</td>\n",
       "      <td>885.63500</td>\n",
       "      <td>916.478420</td>\n",
       "      <td>1041.327760</td>\n",
       "      <td>1107.647360</td>\n",
       "      <td>1107.647360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.280000e+11</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1190.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>601.73502</td>\n",
       "      <td>753.690900</td>\n",
       "      <td>1002.721910</td>\n",
       "      <td>1140.760200</td>\n",
       "      <td>1140.760200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>1030.0</td>\n",
       "      <td>1120.0</td>\n",
       "      <td>1120.0</td>\n",
       "      <td>727.51070</td>\n",
       "      <td>803.424810</td>\n",
       "      <td>999.632220</td>\n",
       "      <td>1097.767940</td>\n",
       "      <td>1097.767940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.290000e+11</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>1210.0</td>\n",
       "      <td>803.83906</td>\n",
       "      <td>853.670470</td>\n",
       "      <td>1025.859460</td>\n",
       "      <td>1178.846430</td>\n",
       "      <td>1178.846430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1136 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0       1       2       3       4          5           6     \\\n",
       "0   1.290000e+11  1140.0  1090.0  1220.0  1180.0  558.30968  708.226080   \n",
       "1   1.280000e+11  1160.0  1090.0  1110.0  1150.0  612.52544  743.132960   \n",
       "2   1.290000e+11  1150.0  1170.0  1180.0  1210.0  575.88264  752.691620   \n",
       "3   1.280000e+11  1110.0  1140.0  1230.0  1190.0  454.81484  636.541470   \n",
       "4   1.280000e+11  1100.0  1090.0  1180.0  1190.0   44.58538  352.586390   \n",
       "5   1.280000e+11  1190.0  1190.0  1200.0  1210.0  661.38204  791.703530   \n",
       "6   1.280000e+11  1140.0  1150.0  1220.0  1180.0  700.17740  816.962350   \n",
       "7   1.290000e+11  1170.0  1090.0  1110.0  1120.0  355.41880  622.715720   \n",
       "8   1.280000e+11  1120.0  1030.0  1160.0  1090.0   34.70296  443.964570   \n",
       "9   1.290000e+11  1180.0  1140.0  1200.0  1250.0  232.14610  550.122420   \n",
       "10  1.280000e+11  1190.0  1070.0  1250.0  1230.0  165.97928  494.665340   \n",
       "11  1.280000e+11  1060.0  1070.0  1130.0  1150.0   27.76778  398.871630   \n",
       "12  1.290000e+11  1130.0   997.0  1140.0  1190.0  309.58848  534.714960   \n",
       "13  1.280000e+11  1100.0  1100.0  1200.0  1200.0  591.59610  729.217430   \n",
       "14  1.280000e+11  1200.0  1110.0  1230.0  1210.0  239.61690  477.211190   \n",
       "15  1.280000e+11   869.0  1180.0  1280.0  1220.0  169.32022  488.109160   \n",
       "16  1.290000e+11  1180.0  1090.0  1110.0  1220.0   62.59544  402.955530   \n",
       "17  1.290000e+11  1140.0  1040.0  1170.0  1200.0  448.80416  681.194930   \n",
       "18  1.280000e+11   767.0  1100.0  1140.0  1140.0  612.49166  729.332930   \n",
       "19  1.290000e+11  1070.0  1120.0  1140.0  1150.0  719.94806  827.902570   \n",
       "20  1.290000e+11  1070.0  1110.0  1200.0  1240.0  363.48800  596.185070   \n",
       "21  1.280000e+11     NaN  1140.0  1270.0  1190.0  173.35914  608.153880   \n",
       "22  1.280000e+11  1130.0  1080.0  1100.0  1180.0  599.86098  756.968120   \n",
       "23  1.290000e+11  1160.0  1090.0  1200.0  1210.0  406.79626  655.088550   \n",
       "24  1.280000e+11  1080.0  1120.0  1240.0  1180.0  188.10744  425.490250   \n",
       "25  1.290000e+11  1160.0  1140.0  1200.0  1180.0  212.65666  598.170500   \n",
       "26  1.290000e+11  1150.0  1120.0  1160.0  1230.0  599.07166  746.813160   \n",
       "27  1.280000e+11  1080.0  1080.0  1220.0  1230.0   29.56814  544.755940   \n",
       "28  1.290000e+11  1150.0  1070.0  1140.0  1100.0  509.00020  676.688720   \n",
       "29  1.280000e+11  1150.0  1130.0  1150.0  1080.0  330.26994  497.478690   \n",
       "..           ...     ...     ...     ...     ...        ...         ...   \n",
       "70  1.280000e+11  1130.0  1100.0  1130.0  1140.0  567.55728  789.215050   \n",
       "71  1.290000e+11  1070.0  1080.0  1130.0  1190.0   48.61642  394.710580   \n",
       "72  1.280000e+11  1110.0  1060.0  1190.0  1180.0   35.30004  459.349300   \n",
       "73  1.290000e+11  1140.0  1060.0  1110.0  1100.0  789.60168  843.854960   \n",
       "74  1.280000e+11  1060.0  1040.0  1150.0  1180.0  486.79352  657.023690   \n",
       "75  1.290000e+11  1170.0  1060.0  1130.0  1120.0  514.34404  718.785770   \n",
       "76  1.290000e+11  1210.0  1120.0  1190.0  1230.0  702.10014  841.772130   \n",
       "77  1.290000e+11  1170.0  1130.0  1190.0  1170.0  302.25906  675.363880   \n",
       "78  1.290000e+11  1120.0  1120.0  1250.0  1230.0  247.43284  563.351620   \n",
       "79  1.290000e+11  1090.0  1090.0  1210.0  1230.0  180.97344  457.798290   \n",
       "80  1.290000e+11  1100.0  1160.0  1140.0  1190.0  614.03690  764.677650   \n",
       "81  1.290000e+11  1080.0  1030.0  1040.0  1130.0  610.63208  704.984400   \n",
       "82  1.280000e+11  1070.0  1080.0  1170.0  1180.0   49.60422  466.893070   \n",
       "83  1.280000e+11  1160.0  1090.0  1200.0  1230.0   35.97522  306.738970   \n",
       "84  1.280000e+11  1120.0  1150.0  1220.0  1180.0  673.04410  763.601830   \n",
       "85  1.290000e+11  1140.0  1070.0  1140.0  1170.0  832.77608  907.869780   \n",
       "86  1.290000e+11  1090.0  1050.0  1230.0  1200.0  215.15820  498.764150   \n",
       "87  1.290000e+11   896.0  1090.0  1190.0  1160.0  484.27250  696.529609   \n",
       "88  1.280000e+11  1090.0  1030.0  1100.0  1140.0  710.46390  784.446420   \n",
       "89  1.280000e+11  1180.0  1140.0  1160.0  1170.0  337.60326  578.371040   \n",
       "90  1.280000e+11  1120.0  1150.0  1170.0  1170.0   47.20836  436.909080   \n",
       "91  1.290000e+11  1140.0  1030.0  1180.0  1210.0   60.51842  838.319240   \n",
       "92  1.280000e+11  1070.0  1010.0  1100.0  1140.0  569.71262  690.929560   \n",
       "93  1.280000e+11   862.0  1190.0  1190.0  1230.0  157.31524  533.311710   \n",
       "94  1.290000e+11  1120.0  1090.0  1200.0  1190.0  430.49524  630.382490   \n",
       "95  1.280000e+11  1160.0  1200.0  1180.0  1180.0   43.93032  316.842060   \n",
       "96  1.280000e+11  1110.0  1030.0  1110.0  1120.0  885.63500  916.478420   \n",
       "97  1.280000e+11  1150.0  1100.0  1190.0  1180.0  601.73502  753.690900   \n",
       "98  1.290000e+11  1080.0  1030.0  1120.0  1120.0  727.51070  803.424810   \n",
       "99  1.290000e+11  1140.0  1020.0  1170.0  1210.0  803.83906  853.670470   \n",
       "\n",
       "           7            8            9     ...   1126  1127  1128  1129  1130  \\\n",
       "0    948.403690  1134.062410  1134.062410  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "1    978.090070  1155.537860  1155.537860  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "2   1063.356990  1175.861690  1175.861690  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "3   1012.651280  1170.493130  1170.493130  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "4    759.125630  1120.094080  1120.094080  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "5   1027.495210  1143.110060  1143.110060  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "6   1079.514640  1176.049290  1176.049290  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "7    909.589260  1076.653750  1076.653750  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "8    920.295860  1105.681600  1105.681600  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "9    900.886520  1145.012660  1145.012660  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "10   892.622170  1146.710520  1146.710520  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "11   878.638950  1151.256090  1151.256090  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "12   844.997670  1139.331930  1139.331930  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "13  1028.781450  1174.291830  1174.291830  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "14  1082.421980  1179.335950  1179.335950  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "15   895.711740  1171.815000  1171.815000  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "16   879.205150  1155.545850  1155.545850  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "17   895.755150  1144.835340  1144.835340  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "18   983.504840  1119.321320  1119.321320  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "19  1022.190300  1139.942710  1139.942710  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "20   952.453150  1148.269720  1148.269720  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "21   851.943180  1140.837740  1140.837740  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "22   930.289100  1095.713300  1095.713300  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "23   889.556670  1137.629790  1137.629790  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "24   786.081590  1058.253120  1058.253120  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "25   982.697410  1134.660150  1134.660150  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "26   971.101060  1137.097310  1137.097310  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "27   953.800690  1170.686600  1170.686600  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "28   955.612560  1101.549150  1101.549150  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "29   862.679560  1116.842050  1116.842050  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "..          ...          ...          ...  ...    ...   ...   ...   ...   ...   \n",
       "70   981.750750  1114.772670  1114.772670  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "71   819.896670  1113.943010  1113.943010  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "72   872.120790  1101.754860  1101.754860  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "73  1014.573840  1123.998720  1123.998720  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "74   970.936480  1153.051340  1153.051340  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "75  1003.381780  1133.072490  1133.072490  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "76  1014.189100  1160.711570  1160.711570  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "77   972.452730  1142.146820  1142.146820  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "78   891.871790  1147.314940  1147.314940  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "79   861.067280  1129.209610  1129.209610  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "80  1035.954150  1183.067300  1183.067300  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "81   955.040470  1130.543990  1130.543990  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "82   836.263250  1123.286660  1123.286660  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "83   815.999450  1114.036320  1114.036320  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "84  1050.309650  1171.815510  1171.815510  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "85  1076.301290  1144.761050  1144.761050  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "86   925.245670  1158.808470  1158.808470  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "87   920.550427  1121.010545  1121.010545  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "88   958.820040  1092.210720  1092.210720  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "89   871.474070  1142.049530  1142.049530  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "90   848.331360  1109.410990  1109.410990  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "91  1030.897330  1160.809520  1160.809520  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "92   932.362970  1076.660950  1076.660950  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "93   917.982670  1176.793010  1176.793010  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "94   911.629650  1151.631660  1151.631660  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "95   912.551180  1143.881630  1143.881630  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "96  1041.327760  1107.647360  1107.647360  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "97  1002.721910  1140.760200  1140.760200  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "98   999.632220  1097.767940  1097.767940  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "99  1025.859460  1178.846430  1178.846430  ...    0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "    1131  1132  1133  1134  1135  \n",
       "0    0.0   0.0   0.0   0.0  56.0  \n",
       "1    0.0   0.0   0.0   0.0  51.0  \n",
       "2    0.0   0.0   0.0   0.0   NaN  \n",
       "3    0.0   0.0   0.0   0.0  37.0  \n",
       "4    0.0   0.0   0.0   0.0  56.0  \n",
       "5    0.0   0.0   0.0   0.0  54.0  \n",
       "6    0.0   0.0   0.0   0.0  51.0  \n",
       "7    0.0   0.0   0.0   0.0   NaN  \n",
       "8    0.0   0.0   0.0   0.0  51.0  \n",
       "9    0.0   0.0   0.0   0.0   NaN  \n",
       "10   0.0   0.0   0.0   0.0  37.0  \n",
       "11   0.0   0.0   0.0   0.0  54.0  \n",
       "12   0.0   0.0   0.0   0.0   NaN  \n",
       "13   0.0   0.0   0.0   0.0  54.0  \n",
       "14   0.0   0.0   0.0   0.0  56.0  \n",
       "15   0.0   0.0   0.0   0.0  51.0  \n",
       "16   0.0   0.0   0.0   0.0  37.0  \n",
       "17   0.0   0.0   0.0   0.0  54.0  \n",
       "18   0.0   0.0   0.0   0.0  51.0  \n",
       "19   0.0   0.0   0.0   0.0  51.0  \n",
       "20   0.0   0.0   0.0   0.0  37.0  \n",
       "21   0.0   0.0   0.0   0.0  54.0  \n",
       "22   0.0   0.0   0.0   0.0  51.0  \n",
       "23   0.0   0.0   0.0   0.0  54.0  \n",
       "24   0.0   0.0   0.0   0.0  54.0  \n",
       "25   0.0   0.0   0.0   0.0  51.0  \n",
       "26   0.0   0.0   0.0   0.0   NaN  \n",
       "27   0.0   0.0   0.0   0.0  51.0  \n",
       "28   0.0   0.0   0.0   0.0   NaN  \n",
       "29   0.0   0.0   0.0   0.0  50.0  \n",
       "..   ...   ...   ...   ...   ...  \n",
       "70   0.0   0.0   0.0   0.0  51.0  \n",
       "71   0.0   0.0   0.0   0.0   NaN  \n",
       "72   0.0   0.0   0.0   0.0  51.0  \n",
       "73   0.0   0.0   0.0   0.0   NaN  \n",
       "74   0.0   0.0   0.0   0.0  37.0  \n",
       "75   0.0   0.0   0.0   0.0   NaN  \n",
       "76   0.0   0.0   0.0   0.0  37.0  \n",
       "77   0.0   0.0   0.0   0.0  50.0  \n",
       "78   0.0   0.0   0.0   0.0   NaN  \n",
       "79   0.0   0.0   0.0   0.0   NaN  \n",
       "80   0.0   0.0   0.0   0.0   NaN  \n",
       "81   0.0   0.0   0.0   0.0  56.0  \n",
       "82   0.0   0.0   0.0   0.0  56.0  \n",
       "83   0.0   0.0   0.0   0.0  56.0  \n",
       "84   0.0   0.0   0.0   0.0  54.0  \n",
       "85   0.0   0.0   0.0   0.0  56.0  \n",
       "86   0.0   0.0   0.0   0.0   NaN  \n",
       "87   0.0   0.0   0.0   0.0  51.0  \n",
       "88   0.0   0.0   0.0   0.0  75.0  \n",
       "89   0.0   0.0   0.0   0.0  54.0  \n",
       "90   0.0   0.0   0.0   0.0  51.0  \n",
       "91   0.0   0.0   0.0   0.0  56.0  \n",
       "92   0.0   0.0   0.0   0.0  75.0  \n",
       "93   0.0   0.0   0.0   0.0  51.0  \n",
       "94   0.0   0.0   0.0   0.0  37.0  \n",
       "95   0.0   0.0   0.0   0.0  36.0  \n",
       "96   0.0   0.0   0.0   0.0  56.0  \n",
       "97   0.0   0.0   0.0   0.0  37.0  \n",
       "98   0.0   0.0   0.0   0.0   NaN  \n",
       "99   0.0   0.0   0.0   0.0  51.0  \n",
       "\n",
       "[100 rows x 1136 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X8 = pd.DataFrame(X_mx)\n",
    "X8[:][0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost 的交叉验证，一般xgb的交叉验证是用来观测那些参数的效果好。然后再用好的参数去train。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#对xgboost模型进行交叉验证，并且画出ROC曲线。\n",
    "def xgboost_cv(X, y, xgboost, Kfold):\n",
    "    random_state = np.random.RandomState(0)\n",
    "    skf = StratifiedKFold(n_splits=Kfold,random_state=random_state) #k fold交叉验证\n",
    "    i=0\n",
    "    xgb_models = []\n",
    "    for train_index, test_index in skf.split(X,y):\n",
    "        xgb_model = xgboost\n",
    "        xgb_model = xgb_model.fit(X[train_index], y[train_index], \n",
    "                                  eval_set=[(X[train_index], y[train_index]),\n",
    "                                            (X[test_index], y[test_index])],\n",
    "                                  eval_metric = \"auc\",\n",
    "                                  verbose = False)\n",
    "\n",
    "        #evals_result = xgb_model.evals_result()\n",
    "        #print(evals_result)\n",
    "        probas_ = xgb_model.predict_proba(X[test_index])\n",
    "\n",
    "        #[:,1]二分类有0的概率，也有预测为1的概率，这里提取预测为1的概率\n",
    "        fpr, tpr, thresholds = roc_curve(y[test_index], probas_[:,1])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        score = roc_auc_score(y[test_index] , probas_[:,1])#验证集的auc分数\n",
    "\n",
    "        train_probas = xgb_model.predict_proba(X[train_index])\n",
    "        train_score = roc_auc_score(y[train_index], train_probas[:,1])#训练集的auc分数\n",
    "        print(\"auc_test: %5f,auc_train:%5f in %d fold. index shape:%d\"\\\n",
    "              %(score, train_score, i, len(train_index))) \n",
    "\n",
    "        plt.plot(fpr, tpr, lw=1, alpha=0.8,\n",
    "                 label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "        i += 1\n",
    "        xgb_models.append(xgb_model)\n",
    "    \n",
    "\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "             label='Luck', alpha=.8)\n",
    "\n",
    "    plt.show()\n",
    "    return xgb_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc_test: 0.906989,auc_train:1.000000 in 0 fold. index shape:89924\n",
      "auc_test: 0.909288,auc_train:1.000000 in 1 fold. index shape:89924\n",
      "auc_test: 0.903501,auc_train:1.000000 in 2 fold. index shape:89924\n",
      "auc_test: 0.897696,auc_train:1.000000 in 3 fold. index shape:89924\n",
      "auc_test: 0.872908,auc_train:1.000000 in 4 fold. index shape:89924\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4t3MAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fef6e61d710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb_model_cv = xgb.XGBClassifier(booster='gbtree',\n",
    "                              silent=False,\n",
    "                              #n_jobs=5, #不设置的话，自动获得最大线程数\n",
    "                              #以上为general params\n",
    "                              \n",
    "                              learning_rate = 0.1,#在xgboost的package中等价于eta参数\n",
    "                              min_child_weight = 2, #控制过拟合，越大越不会过拟合\n",
    "                              \n",
    "                              max_depth=100, #控制过拟合，越小越不会过拟合\n",
    "                              max_delta_step = 1, #数据不均衡的时候可以用\n",
    "                              gamma = 0,         #模型在默认情况下，对于一个节点的划分\n",
    "                                                 #只有在其loss function 得到结果大于0的情况下才进行，\n",
    "                                                 #而gamma 给定了所需的最低loss function的值.\n",
    "                                                 #所以gamma越大越保守（conservation）\n",
    "                              subsample = 0.8,    #太大会过拟合，太小会欠拟合\n",
    "                              colsample_bytree=0.8, #\n",
    "                              reg_lambda = 100, #L2正则化的权重参数,给linear模型用的。gbtree不用\n",
    "                              reg_alpha = 0, #L1正则化的权重参数,给linear模型用的。gbtree不用\n",
    "                              scale_pos_weight=340,\n",
    "                              #以上是booster的参数\n",
    "                              \n",
    "                              \n",
    "                              random_state = 0,\n",
    "                              n_estimators = 200)#树的棵树\n",
    "\n",
    "\n",
    "#xgbModels = xgboost_cv(X, y, xgb_model_cv, 5)\n",
    "xgbModels = xgboost_cv(X5, y5, xgb_model_cv, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc_test: 0.905504,auc_train:1.000000 in 0 fold. index shape:89924\n",
      "auc_test: 0.915356,auc_train:1.000000 in 1 fold. index shape:89924\n",
      "auc_test: 0.903964,auc_train:1.000000 in 2 fold. index shape:89924\n",
      "auc_test: 0.891250,auc_train:1.000000 in 3 fold. index shape:89924\n",
      "auc_test: 0.884471,auc_train:1.000000 in 4 fold. index shape:89924\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQQp3EZEIUriLiETQ/wdG5AatWDyjjwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbb96803e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb_model_cv1 = xgb.XGBClassifier(booster='gbtree',\n",
    "                              silent=False,\n",
    "                              #n_jobs=5, #不设置的话，自动获得最大线程数\n",
    "                              #以上为general params\n",
    "                              \n",
    "                              learning_rate = 0.1,#在xgboost的package中等价于eta参数\n",
    "                              min_child_weight = 2, #控制过拟合，越大越不会过拟合\n",
    "                              \n",
    "                              max_depth=100, #控制过拟合，越小越不会过拟合\n",
    "                              max_delta_step = 1, #数据不均衡的时候可以用\n",
    "                              gamma = 0,         #模型在默认情况下，对于一个节点的划分\n",
    "                                                 #只有在其loss function 得到结果大于0的情况下才进行，\n",
    "                                                 #而gamma 给定了所需的最低loss function的值.\n",
    "                                                 #所以gamma越大越保守（conservation）\n",
    "                              subsample = 0.8,    #太大会过拟合，太小会欠拟合\n",
    "                              colsample_bytree=0.8, #\n",
    "                              reg_lambda = 100, #L2正则化的权重参数,给linear模型用的。gbtree不用\n",
    "                              reg_alpha = 0, #L1正则化的权重参数,给linear模型用的。gbtree不用\n",
    "                              scale_pos_weight=340,\n",
    "                              #以上是booster的参数\n",
    "                              \n",
    "                              \n",
    "                              random_state = 0,\n",
    "                              n_estimators = 200)#树的棵树\n",
    "\n",
    "\n",
    "xgbModels1 = xgboost_cv(X, y, xgb_model_cv1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc_test: 0.916228,auc_train:1.000000 in 0 fold. index shape:89924\n",
      "auc_test: 0.926215,auc_train:1.000000 in 1 fold. index shape:89924\n",
      "auc_test: 0.902811,auc_train:1.000000 in 2 fold. index shape:89924\n",
      "auc_test: 0.893152,auc_train:1.000000 in 3 fold. index shape:89924\n",
      "auc_test: 0.879686,auc_train:1.000000 in 4 fold. index shape:89924\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbb976d3400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb_model_cv2 = xgb.XGBClassifier(booster='gbtree',\n",
    "                              silent=True,\n",
    "                              #n_jobs=5, #不设置的话，自动获得最大线程数\n",
    "                              #以上为general params\n",
    "                              \n",
    "                              learning_rate = 0.1,#在xgboost的package中等价于eta参数\n",
    "                              min_child_weight = 2, #控制过拟合，越大越不会过拟合\n",
    "                              \n",
    "                              max_depth=50, #控制过拟合，越小越不会过拟合\n",
    "                              max_delta_step = 1, #数据不均衡的时候可以用\n",
    "                              gamma = 0,         #模型在默认情况下，对于一个节点的划分\n",
    "                                                 #只有在其loss function 得到结果大于0的情况下才进行，\n",
    "                                                 #而gamma 给定了所需的最低loss function的值.\n",
    "                                                 #所以gamma越大越保守（conservation）\n",
    "                              subsample = 0.8,    #太大会过拟合，太小会欠拟合\n",
    "                              colsample_bytree=0.8, #\n",
    "                              reg_lambda = 100, #L2正则化的权重参数,给linear模型用的。gbtree不用\n",
    "                              reg_alpha = 0, #L1正则化的权重参数,给linear模型用的。gbtree不用\n",
    "                              scale_pos_weight=340,\n",
    "                              #以上是booster的参数\n",
    "                              \n",
    "                              \n",
    "                              random_state = 0,\n",
    "                              n_estimators = 200)#树的棵树\n",
    "\n",
    "\n",
    "xgbModels2 = xgboost_cv(X, y, xgb_model_cv2, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对交叉验证得到的模型，将它们用于测试集，并将平均得到的结果保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib #jbolib模块\n",
    "#将xgboost交叉验证的几个模型去预测测试集，并且对其取平均\n",
    "def testModel(models, testX, versionSaved='100'):\n",
    "    i=0\n",
    "    probas = []\n",
    "    for xgb_final in models:\n",
    "        predict = xgb_final.predict(testX)\n",
    "        #查看有多少样本被预测为1\n",
    "        print('samples have been predicted as positive samples: ', \\\n",
    "              sum(predict),'in %d model'%(i))\n",
    "        probas_ = xgb_final.predict_proba(testX)\n",
    "        probas.append(probas_[:,1])\n",
    "        i+=1\n",
    "\n",
    "    print(type(probas))\n",
    "    print(len(probas), len(probas[0]))\n",
    "    predict = np.sum(probas, axis=0)/len(probas)\n",
    "    #保存测试集的预测结果\n",
    "    result = pd.read_csv('./originalDataset/exampleSubmission.csv')\n",
    "    result.label = predict\n",
    "    result.to_csv('./outputs/submission%s.csv'%(versionSaved), index=False)\n",
    "    \n",
    "    #joblib.dump(xgb_final, './models/xgbModel_%s.model'%(versionSaved))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299', 'f300', 'f301', 'f302', 'f303', 'f304', 'f305', 'f306', 'f307', 'f308', 'f309', 'f310', 'f311', 'f312', 'f313', 'f314', 'f315', 'f316', 'f317', 'f318', 'f319', 'f320', 'f321', 'f322', 'f323', 'f324', 'f325', 'f326', 'f327', 'f328', 'f329', 'f330', 'f331', 'f332', 'f333', 'f334', 'f335', 'f336', 'f337', 'f338', 'f339', 'f340', 'f341', 'f342', 'f343', 'f344', 'f345', 'f346', 'f347', 'f348', 'f349', 'f350', 'f351', 'f352', 'f353', 'f354', 'f355', 'f356', 'f357', 'f358', 'f359', 'f360', 'f361', 'f362', 'f363', 'f364', 'f365', 'f366', 'f367', 'f368', 'f369', 'f370', 'f371', 'f372', 'f373', 'f374', 'f375', 'f376', 'f377', 'f378', 'f379', 'f380', 'f381', 'f382', 'f383', 'f384', 'f385', 'f386', 'f387', 'f388', 'f389', 'f390', 'f391', 'f392', 'f393', 'f394', 'f395', 'f396', 'f397', 'f398', 'f399', 'f400', 'f401', 'f402', 'f403', 'f404', 'f405', 'f406', 'f407', 'f408', 'f409', 'f410', 'f411', 'f412', 'f413', 'f414', 'f415', 'f416', 'f417', 'f418', 'f419', 'f420', 'f421', 'f422', 'f423', 'f424', 'f425', 'f426', 'f427', 'f428', 'f429', 'f430', 'f431', 'f432', 'f433', 'f434', 'f435', 'f436', 'f437', 'f438', 'f439', 'f440', 'f441', 'f442', 'f443', 'f444', 'f445', 'f446', 'f447', 'f448', 'f449', 'f450', 'f451', 'f452', 'f453', 'f454', 'f455', 'f456', 'f457', 'f458', 'f459', 'f460', 'f461', 'f462', 'f463', 'f464', 'f465', 'f466', 'f467', 'f468', 'f469', 'f470', 'f471', 'f472', 'f473', 'f474', 'f475', 'f476', 'f477', 'f478', 'f479', 'f480', 'f481', 'f482', 'f483', 'f484', 'f485', 'f486', 'f487', 'f488', 'f489', 'f490', 'f491', 'f492', 'f493', 'f494', 'f495', 'f496', 'f497', 'f498', 'f499', 'f500', 'f501', 'f502', 'f503', 'f504', 'f505', 'f506', 'f507', 'f508', 'f509', 'f510', 'f511', 'f512', 'f513', 'f514', 'f515', 'f516', 'f517', 'f518', 'f519', 'f520', 'f521', 'f522', 'f523', 'f524', 'f525', 'f526', 'f527', 'f528', 'f529', 'f530', 'f531', 'f532', 'f533', 'f534', 'f535', 'f536', 'f537', 'f538', 'f539', 'f540', 'f541', 'f542', 'f543', 'f544', 'f545', 'f546', 'f547', 'f548', 'f549', 'f550', 'f551', 'f552', 'f553', 'f554', 'f555', 'f556', 'f557', 'f558', 'f559', 'f560', 'f561', 'f562', 'f563', 'f564', 'f565', 'f566', 'f567', 'f568', 'f569', 'f570', 'f571', 'f572', 'f573', 'f574', 'f575', 'f576', 'f577', 'f578', 'f579', 'f580', 'f581', 'f582', 'f583', 'f584', 'f585', 'f586', 'f587', 'f588', 'f589', 'f590', 'f591', 'f592', 'f593', 'f594', 'f595', 'f596', 'f597', 'f598', 'f599', 'f600', 'f601', 'f602', 'f603', 'f604', 'f605', 'f606', 'f607', 'f608', 'f609', 'f610', 'f611', 'f612', 'f613', 'f614', 'f615', 'f616', 'f617', 'f618', 'f619', 'f620', 'f621', 'f622', 'f623', 'f624', 'f625', 'f626', 'f627', 'f628', 'f629', 'f630', 'f631', 'f632', 'f633', 'f634', 'f635', 'f636', 'f637', 'f638', 'f639', 'f640', 'f641', 'f642', 'f643', 'f644', 'f645', 'f646', 'f647', 'f648', 'f649', 'f650', 'f651', 'f652', 'f653', 'f654', 'f655', 'f656', 'f657', 'f658', 'f659', 'f660', 'f661', 'f662', 'f663', 'f664', 'f665', 'f666', 'f667', 'f668', 'f669', 'f670', 'f671', 'f672', 'f673', 'f674', 'f675', 'f676', 'f677', 'f678', 'f679', 'f680', 'f681', 'f682', 'f683', 'f684', 'f685', 'f686', 'f687', 'f688', 'f689', 'f690', 'f691', 'f692', 'f693', 'f694', 'f695', 'f696', 'f697', 'f698', 'f699', 'f700', 'f701', 'f702', 'f703', 'f704', 'f705', 'f706', 'f707', 'f708', 'f709', 'f710', 'f711', 'f712', 'f713', 'f714', 'f715', 'f716', 'f717', 'f718', 'f719', 'f720', 'f721', 'f722', 'f723', 'f724', 'f725', 'f726', 'f727', 'f728', 'f729', 'f730', 'f731', 'f732', 'f733', 'f734', 'f735', 'f736', 'f737', 'f738', 'f739', 'f740', 'f741', 'f742', 'f743', 'f744', 'f745', 'f746', 'f747', 'f748', 'f749', 'f750', 'f751', 'f752', 'f753', 'f754', 'f755', 'f756', 'f757', 'f758', 'f759', 'f760', 'f761', 'f762', 'f763', 'f764', 'f765', 'f766', 'f767', 'f768', 'f769', 'f770', 'f771', 'f772', 'f773', 'f774', 'f775', 'f776', 'f777', 'f778', 'f779', 'f780', 'f781', 'f782', 'f783', 'f784', 'f785', 'f786', 'f787', 'f788', 'f789', 'f790', 'f791', 'f792', 'f793', 'f794', 'f795', 'f796', 'f797', 'f798', 'f799', 'f800', 'f801', 'f802', 'f803', 'f804', 'f805', 'f806', 'f807', 'f808', 'f809', 'f810', 'f811', 'f812', 'f813', 'f814', 'f815', 'f816', 'f817', 'f818', 'f819', 'f820', 'f821', 'f822', 'f823', 'f824', 'f825', 'f826', 'f827', 'f828', 'f829', 'f830', 'f831', 'f832', 'f833', 'f834', 'f835', 'f836', 'f837', 'f838', 'f839', 'f840', 'f841', 'f842', 'f843', 'f844', 'f845', 'f846', 'f847', 'f848', 'f849', 'f850', 'f851', 'f852', 'f853', 'f854', 'f855', 'f856', 'f857', 'f858', 'f859', 'f860', 'f861', 'f862', 'f863', 'f864', 'f865', 'f866', 'f867', 'f868', 'f869', 'f870', 'f871', 'f872', 'f873', 'f874']\ntraining data did not have the following fields: f599, f396, f474, f788, f383, f203, f744, f357, f251, f208, f739, f830, f604, f404, f457, f537, f650, f790, f513, f220, f377, f676, f848, f268, f493, f371, f318, f441, f381, f480, f244, f527, f284, f279, f385, f704, f168, f431, f321, f714, f173, f442, f627, f271, f545, f308, f610, f314, f196, f820, f815, f557, f706, f467, f446, f312, f850, f752, f153, f656, f436, f747, f172, f164, f833, f776, f634, f520, f405, f390, f645, f753, f563, f490, f485, f288, f609, f771, f798, f618, f647, f866, f695, f238, f456, f813, f817, f740, f195, f703, f745, f249, f177, f255, f302, f725, f199, f240, f510, f666, f234, f236, f591, f187, f795, f834, f503, f421, f843, f709, f150, f333, f317, f352, f746, f202, f533, f707, f328, f737, f486, f829, f816, f261, f281, f235, f764, f398, f407, f657, f605, f188, f861, f554, f601, f653, f304, f570, f497, f433, f269, f325, f568, f665, f430, f579, f587, f690, f607, f451, f411, f237, f169, f276, f499, f285, f330, f362, f369, f760, f823, f826, f160, f724, f654, f534, f459, f414, f683, f438, f287, f874, f697, f730, f784, f519, f869, f468, f491, f589, f386, f827, f322, f667, f374, f792, f722, f822, f418, f152, f399, f477, f345, f621, f323, f832, f853, f254, f370, f623, f791, f415, f870, f245, f257, f409, f482, f720, f594, f576, f300, f726, f797, f207, f313, f800, f439, f223, f831, f299, f810, f498, f765, f651, f239, f721, f403, f511, f259, f643, f523, f644, f819, f229, f807, f337, f713, f410, f593, f748, f293, f639, f521, f394, f228, f339, f171, f267, f243, f872, f422, f380, f757, f551, f783, f219, f546, f566, f673, f649, f775, f423, f620, f659, f759, f327, f526, f715, f332, f640, f280, f613, f808, f868, f571, f572, f575, f638, f462, f774, f361, f671, f799, f578, f402, f710, f505, f793, f818, f258, f213, f373, f201, f558, f590, f161, f200, f574, f606, f506, f334, f387, f227, f464, f450, f193, f628, f509, f163, f741, f824, f567, f846, f750, f608, f233, f805, f693, f427, f461, f857, f742, f751, f248, f532, f286, f368, f560, f700, f413, f777, f524, f489, f272, f858, f155, f166, f273, f283, f343, f738, f230, f347, f763, f863, f842, f384, f541, f192, f274, f452, f206, f372, f531, f655, f680, f762, f767, f641, f768, f167, f719, f844, f226, f158, f264, f290, f684, f779, f211, f787, f512, f495, f253, f295, f186, f809, f151, f191, f307, f625, f670, f556, f522, f338, f851, f736, f756, f803, f550, f342, f247, f326, f565, f408, f836, f766, f547, f549, f615, f504, f663, f354, f838, f429, f440, f426, f837, f660, f603, f632, f205, f664, f755, f246, f222, f681, f781, f581, f159, f555, f539, f543, f364, f501, f367, f225, f692, f315, f218, f661, f802, f642, f770, f617, f419, f463, f542, f569, f185, f319, f382, f483, f500, f624, f270, f252, f379, f806, f841, f701, f465, f801, f691, f782, f631, f622, f772, f585, f432, f518, f190, f366, f311, f672, f675, f375, f359, f282, f530, f198, f708, f278, f735, f514, f794, f400, f397, f355, f174, f696, f864, f743, f811, f702, f178, f303, f552, f391, f320, f215, f840, f305, f785, f658, f536, f346, f351, f758, f242, f445, f435, f561, f182, f296, f749, f289, f559, f475, f487, f291, f825, f224, f453, f183, f619, f417, f476, f716, f614, f867, f294, f471, f157, f630, f204, f611, f508, f306, f689, f194, f873, f595, f712, f582, f688, f629, f773, f156, f292, f335, f458, f447, f162, f231, f454, f525, f698, f845, f348, f682, f184, f181, f341, f434, f584, f731, f528, f401, f470, f646, f648, f210, f780, f544, f209, f839, f732, f694, f849, f761, f812, f679, f507, f389, f496, f277, f221, f449, f535, f728, f492, f852, f425, f669, f592, f358, f256, f360, f515, f598, f678, f562, f633, f860, f455, f420, f232, f406, f705, f821, f538, f265, f600, f553, f734, f612, f460, f262, f329, f733, f154, f472, f149, f855, f859, f298, f686, f473, f588, f395, f336, f856, f484, f786, f469, f769, f393, f835, f729, f363, f871, f392, f344, f214, f443, f378, f616, f189, f517, f636, f175, f217, f176, f814, f212, f586, f583, f828, f577, f494, f297, f718, f685, f865, f573, f754, f540, f626, f331, f388, f727, f548, f580, f711, f789, f180, f674, f637, f428, f448, f412, f263, f416, f778, f862, f350, f275, f516, f301, f310, f424, f488, f596, f804, f602, f179, f266, f309, f353, f662, f349, f479, f170, f365, f597, f717, f652, f677, f529, f340, f250, f502, f376, f444, f197, f356, f847, f796, f241, f466, f216, f564, f165, f854, f687, f668, f260, f316, f699, f481, f478, f324, f635, f723, f437",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-22c5c7982b1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtestModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclf2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'875ensemble'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-786269a4e167>\u001b[0m in \u001b[0;36mtestModel\u001b[0;34m(models, versionSaved)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprobas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mxgb_final\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;31m#查看有多少样本被预测为1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'samples have been predicted as positive samples: '\u001b[0m\u001b[0;34m,\u001b[0m               \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'in %d model'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/liwb/anaconda3/lib/python3.5/site-packages/xgboost-0.6-py3.5.egg/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, output_margin, ntree_limit)\u001b[0m\n\u001b[1;32m    525\u001b[0m         class_probs = self.get_booster().predict(test_dmatrix,\n\u001b[1;32m    526\u001b[0m                                                  \u001b[0moutput_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_margin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m                                                  ntree_limit=ntree_limit)\n\u001b[0m\u001b[1;32m    528\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0mcolumn_indexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/liwb/anaconda3/lib/python3.5/site-packages/xgboost-0.6-py3.5.egg/xgboost/core.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, output_margin, ntree_limit, pred_leaf, pred_contribs, approx_contribs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0moption_mask\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0;36m0x08\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_bst_ulong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/liwb/anaconda3/lib/python3.5/site-packages/xgboost-0.6-py3.5.egg/xgboost/core.py\u001b[0m in \u001b[0;36m_validate_features\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m                 raise ValueError(msg.format(self.feature_names,\n\u001b[0;32m-> 1286\u001b[0;31m                                             data.feature_names))\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_split_value_histogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_pandas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: feature_names mismatch: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299', 'f300', 'f301', 'f302', 'f303', 'f304', 'f305', 'f306', 'f307', 'f308', 'f309', 'f310', 'f311', 'f312', 'f313', 'f314', 'f315', 'f316', 'f317', 'f318', 'f319', 'f320', 'f321', 'f322', 'f323', 'f324', 'f325', 'f326', 'f327', 'f328', 'f329', 'f330', 'f331', 'f332', 'f333', 'f334', 'f335', 'f336', 'f337', 'f338', 'f339', 'f340', 'f341', 'f342', 'f343', 'f344', 'f345', 'f346', 'f347', 'f348', 'f349', 'f350', 'f351', 'f352', 'f353', 'f354', 'f355', 'f356', 'f357', 'f358', 'f359', 'f360', 'f361', 'f362', 'f363', 'f364', 'f365', 'f366', 'f367', 'f368', 'f369', 'f370', 'f371', 'f372', 'f373', 'f374', 'f375', 'f376', 'f377', 'f378', 'f379', 'f380', 'f381', 'f382', 'f383', 'f384', 'f385', 'f386', 'f387', 'f388', 'f389', 'f390', 'f391', 'f392', 'f393', 'f394', 'f395', 'f396', 'f397', 'f398', 'f399', 'f400', 'f401', 'f402', 'f403', 'f404', 'f405', 'f406', 'f407', 'f408', 'f409', 'f410', 'f411', 'f412', 'f413', 'f414', 'f415', 'f416', 'f417', 'f418', 'f419', 'f420', 'f421', 'f422', 'f423', 'f424', 'f425', 'f426', 'f427', 'f428', 'f429', 'f430', 'f431', 'f432', 'f433', 'f434', 'f435', 'f436', 'f437', 'f438', 'f439', 'f440', 'f441', 'f442', 'f443', 'f444', 'f445', 'f446', 'f447', 'f448', 'f449', 'f450', 'f451', 'f452', 'f453', 'f454', 'f455', 'f456', 'f457', 'f458', 'f459', 'f460', 'f461', 'f462', 'f463', 'f464', 'f465', 'f466', 'f467', 'f468', 'f469', 'f470', 'f471', 'f472', 'f473', 'f474', 'f475', 'f476', 'f477', 'f478', 'f479', 'f480', 'f481', 'f482', 'f483', 'f484', 'f485', 'f486', 'f487', 'f488', 'f489', 'f490', 'f491', 'f492', 'f493', 'f494', 'f495', 'f496', 'f497', 'f498', 'f499', 'f500', 'f501', 'f502', 'f503', 'f504', 'f505', 'f506', 'f507', 'f508', 'f509', 'f510', 'f511', 'f512', 'f513', 'f514', 'f515', 'f516', 'f517', 'f518', 'f519', 'f520', 'f521', 'f522', 'f523', 'f524', 'f525', 'f526', 'f527', 'f528', 'f529', 'f530', 'f531', 'f532', 'f533', 'f534', 'f535', 'f536', 'f537', 'f538', 'f539', 'f540', 'f541', 'f542', 'f543', 'f544', 'f545', 'f546', 'f547', 'f548', 'f549', 'f550', 'f551', 'f552', 'f553', 'f554', 'f555', 'f556', 'f557', 'f558', 'f559', 'f560', 'f561', 'f562', 'f563', 'f564', 'f565', 'f566', 'f567', 'f568', 'f569', 'f570', 'f571', 'f572', 'f573', 'f574', 'f575', 'f576', 'f577', 'f578', 'f579', 'f580', 'f581', 'f582', 'f583', 'f584', 'f585', 'f586', 'f587', 'f588', 'f589', 'f590', 'f591', 'f592', 'f593', 'f594', 'f595', 'f596', 'f597', 'f598', 'f599', 'f600', 'f601', 'f602', 'f603', 'f604', 'f605', 'f606', 'f607', 'f608', 'f609', 'f610', 'f611', 'f612', 'f613', 'f614', 'f615', 'f616', 'f617', 'f618', 'f619', 'f620', 'f621', 'f622', 'f623', 'f624', 'f625', 'f626', 'f627', 'f628', 'f629', 'f630', 'f631', 'f632', 'f633', 'f634', 'f635', 'f636', 'f637', 'f638', 'f639', 'f640', 'f641', 'f642', 'f643', 'f644', 'f645', 'f646', 'f647', 'f648', 'f649', 'f650', 'f651', 'f652', 'f653', 'f654', 'f655', 'f656', 'f657', 'f658', 'f659', 'f660', 'f661', 'f662', 'f663', 'f664', 'f665', 'f666', 'f667', 'f668', 'f669', 'f670', 'f671', 'f672', 'f673', 'f674', 'f675', 'f676', 'f677', 'f678', 'f679', 'f680', 'f681', 'f682', 'f683', 'f684', 'f685', 'f686', 'f687', 'f688', 'f689', 'f690', 'f691', 'f692', 'f693', 'f694', 'f695', 'f696', 'f697', 'f698', 'f699', 'f700', 'f701', 'f702', 'f703', 'f704', 'f705', 'f706', 'f707', 'f708', 'f709', 'f710', 'f711', 'f712', 'f713', 'f714', 'f715', 'f716', 'f717', 'f718', 'f719', 'f720', 'f721', 'f722', 'f723', 'f724', 'f725', 'f726', 'f727', 'f728', 'f729', 'f730', 'f731', 'f732', 'f733', 'f734', 'f735', 'f736', 'f737', 'f738', 'f739', 'f740', 'f741', 'f742', 'f743', 'f744', 'f745', 'f746', 'f747', 'f748', 'f749', 'f750', 'f751', 'f752', 'f753', 'f754', 'f755', 'f756', 'f757', 'f758', 'f759', 'f760', 'f761', 'f762', 'f763', 'f764', 'f765', 'f766', 'f767', 'f768', 'f769', 'f770', 'f771', 'f772', 'f773', 'f774', 'f775', 'f776', 'f777', 'f778', 'f779', 'f780', 'f781', 'f782', 'f783', 'f784', 'f785', 'f786', 'f787', 'f788', 'f789', 'f790', 'f791', 'f792', 'f793', 'f794', 'f795', 'f796', 'f797', 'f798', 'f799', 'f800', 'f801', 'f802', 'f803', 'f804', 'f805', 'f806', 'f807', 'f808', 'f809', 'f810', 'f811', 'f812', 'f813', 'f814', 'f815', 'f816', 'f817', 'f818', 'f819', 'f820', 'f821', 'f822', 'f823', 'f824', 'f825', 'f826', 'f827', 'f828', 'f829', 'f830', 'f831', 'f832', 'f833', 'f834', 'f835', 'f836', 'f837', 'f838', 'f839', 'f840', 'f841', 'f842', 'f843', 'f844', 'f845', 'f846', 'f847', 'f848', 'f849', 'f850', 'f851', 'f852', 'f853', 'f854', 'f855', 'f856', 'f857', 'f858', 'f859', 'f860', 'f861', 'f862', 'f863', 'f864', 'f865', 'f866', 'f867', 'f868', 'f869', 'f870', 'f871', 'f872', 'f873', 'f874']\ntraining data did not have the following fields: f599, f396, f474, f788, f383, f203, f744, f357, f251, f208, f739, f830, f604, f404, f457, f537, f650, f790, f513, f220, f377, f676, f848, f268, f493, f371, f318, f441, f381, f480, f244, f527, f284, f279, f385, f704, f168, f431, f321, f714, f173, f442, f627, f271, f545, f308, f610, f314, f196, f820, f815, f557, f706, f467, f446, f312, f850, f752, f153, f656, f436, f747, f172, f164, f833, f776, f634, f520, f405, f390, f645, f753, f563, f490, f485, f288, f609, f771, f798, f618, f647, f866, f695, f238, f456, f813, f817, f740, f195, f703, f745, f249, f177, f255, f302, f725, f199, f240, f510, f666, f234, f236, f591, f187, f795, f834, f503, f421, f843, f709, f150, f333, f317, f352, f746, f202, f533, f707, f328, f737, f486, f829, f816, f261, f281, f235, f764, f398, f407, f657, f605, f188, f861, f554, f601, f653, f304, f570, f497, f433, f269, f325, f568, f665, f430, f579, f587, f690, f607, f451, f411, f237, f169, f276, f499, f285, f330, f362, f369, f760, f823, f826, f160, f724, f654, f534, f459, f414, f683, f438, f287, f874, f697, f730, f784, f519, f869, f468, f491, f589, f386, f827, f322, f667, f374, f792, f722, f822, f418, f152, f399, f477, f345, f621, f323, f832, f853, f254, f370, f623, f791, f415, f870, f245, f257, f409, f482, f720, f594, f576, f300, f726, f797, f207, f313, f800, f439, f223, f831, f299, f810, f498, f765, f651, f239, f721, f403, f511, f259, f643, f523, f644, f819, f229, f807, f337, f713, f410, f593, f748, f293, f639, f521, f394, f228, f339, f171, f267, f243, f872, f422, f380, f757, f551, f783, f219, f546, f566, f673, f649, f775, f423, f620, f659, f759, f327, f526, f715, f332, f640, f280, f613, f808, f868, f571, f572, f575, f638, f462, f774, f361, f671, f799, f578, f402, f710, f505, f793, f818, f258, f213, f373, f201, f558, f590, f161, f200, f574, f606, f506, f334, f387, f227, f464, f450, f193, f628, f509, f163, f741, f824, f567, f846, f750, f608, f233, f805, f693, f427, f461, f857, f742, f751, f248, f532, f286, f368, f560, f700, f413, f777, f524, f489, f272, f858, f155, f166, f273, f283, f343, f738, f230, f347, f763, f863, f842, f384, f541, f192, f274, f452, f206, f372, f531, f655, f680, f762, f767, f641, f768, f167, f719, f844, f226, f158, f264, f290, f684, f779, f211, f787, f512, f495, f253, f295, f186, f809, f151, f191, f307, f625, f670, f556, f522, f338, f851, f736, f756, f803, f550, f342, f247, f326, f565, f408, f836, f766, f547, f549, f615, f504, f663, f354, f838, f429, f440, f426, f837, f660, f603, f632, f205, f664, f755, f246, f222, f681, f781, f581, f159, f555, f539, f543, f364, f501, f367, f225, f692, f315, f218, f661, f802, f642, f770, f617, f419, f463, f542, f569, f185, f319, f382, f483, f500, f624, f270, f252, f379, f806, f841, f701, f465, f801, f691, f782, f631, f622, f772, f585, f432, f518, f190, f366, f311, f672, f675, f375, f359, f282, f530, f198, f708, f278, f735, f514, f794, f400, f397, f355, f174, f696, f864, f743, f811, f702, f178, f303, f552, f391, f320, f215, f840, f305, f785, f658, f536, f346, f351, f758, f242, f445, f435, f561, f182, f296, f749, f289, f559, f475, f487, f291, f825, f224, f453, f183, f619, f417, f476, f716, f614, f867, f294, f471, f157, f630, f204, f611, f508, f306, f689, f194, f873, f595, f712, f582, f688, f629, f773, f156, f292, f335, f458, f447, f162, f231, f454, f525, f698, f845, f348, f682, f184, f181, f341, f434, f584, f731, f528, f401, f470, f646, f648, f210, f780, f544, f209, f839, f732, f694, f849, f761, f812, f679, f507, f389, f496, f277, f221, f449, f535, f728, f492, f852, f425, f669, f592, f358, f256, f360, f515, f598, f678, f562, f633, f860, f455, f420, f232, f406, f705, f821, f538, f265, f600, f553, f734, f612, f460, f262, f329, f733, f154, f472, f149, f855, f859, f298, f686, f473, f588, f395, f336, f856, f484, f786, f469, f769, f393, f835, f729, f363, f871, f392, f344, f214, f443, f378, f616, f189, f517, f636, f175, f217, f176, f814, f212, f586, f583, f828, f577, f494, f297, f718, f685, f865, f573, f754, f540, f626, f331, f388, f727, f548, f580, f711, f789, f180, f674, f637, f428, f448, f412, f263, f416, f778, f862, f350, f275, f516, f301, f310, f424, f488, f596, f804, f602, f179, f266, f309, f353, f662, f349, f479, f170, f365, f597, f717, f652, f677, f529, f340, f250, f502, f376, f444, f197, f356, f847, f796, f241, f466, f216, f564, f165, f854, f687, f668, f260, f316, f699, f481, f478, f324, f635, f723, f437"
     ]
    }
   ],
   "source": [
    "testModel([clf2], testX, '875ensemble')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\n",
    "### 训练模型，并且使用测试集预测。将结果保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib #jbolib模块\n",
    "\n",
    "#训练模型，并且用测试集测试，得到预测结果，保存预测结果\n",
    "def xgboostTrain(X, y, testX, xgb_final, versionSaved='100', params=None):\n",
    "    xgb_final = xgb_final.fit(X, y, \n",
    "                     eval_set=[(X, y)],\n",
    "                     eval_metric = \"auc\",\n",
    "                     verbose = False)\n",
    "    \n",
    "    print(xgb_final.evals_result())\n",
    "    predict = xgb_final.predict(testX)\n",
    "    #查看有多少样本被预测为1\n",
    "    print(sum(predict),' samples have been predicted as positive samples')\n",
    "    probas_ = xgb_final.predict_proba(testX)\n",
    "    probas = probas_[:,1]\n",
    "    \n",
    "    #保存测试集的预测结果\n",
    "    result = pd.read_csv('./originalDataset/exampleSubmission.csv')\n",
    "    result.label = probas\n",
    "    result.to_csv('./outputs/submission%s.csv'%(versionSaved), index=False)\n",
    "    \n",
    "    #保存模型\n",
    "    joblib.dump(xgb_final, './models/xgbModel_%s.model'%(versionSaved))\n",
    "    #clf2 = joblib.load('./models/xgb_0501test2.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'validation_0': {'auc': [0.930371, 0.98661, 0.992024, 0.994209, 0.996645, 0.996554, 0.997383, 0.998314, 0.998789, 0.999085, 0.999282, 0.999357, 0.99947, 0.999582, 0.999644, 0.999699, 0.999742, 0.999765, 0.999774, 0.999782, 0.999783, 0.999802, 0.999808, 0.999819, 0.99982, 0.999826, 0.99984, 0.999848, 0.999861, 0.999862, 0.999866, 0.999872, 0.999878, 0.999891, 0.999893, 0.999893, 0.999905, 0.99991, 0.999913, 0.999917, 0.999922, 0.999926, 0.999931, 0.999937, 0.999941, 0.999942, 0.999946, 0.999949, 0.999952, 0.999955, 0.999959, 0.999965, 0.999967, 0.99997, 0.999973, 0.999975, 0.999978, 0.99998, 0.99998, 0.999982, 0.999983, 0.999983, 0.999986, 0.999986, 0.999988, 0.999988, 0.99999, 0.999991, 0.999992, 0.999993, 0.999993, 0.999993, 0.999994, 0.999993, 0.999993, 0.999994, 0.999994, 0.999994, 0.999995, 0.999995, 0.999995, 0.999995, 0.999995, 0.999996, 0.999996, 0.999997, 0.999997, 0.999997, 0.999997, 0.999997, 0.999997, 0.999997, 0.999997, 0.999998, 0.999998, 0.999998, 0.999998, 0.999998, 0.999999, 0.999999, 0.999999, 0.999999, 0.999999, 0.999999, 0.999999, 0.999999, 0.999999, 0.999999, 0.999999, 0.999999, 0.999999, 0.999999, 0.999999, 0.999999, 0.999999, 0.999999, 0.999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}\n",
      "57.0  samples have been predicted as positive samples\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./originalDataset/exampleSubmission.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5b890b0c1633>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#xgboostTrain(X, y, testX, xgb_final, versionSaved= '200depth')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mxgboostTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX_mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversionSaved\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'0501_mx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-50aaed249bf1>\u001b[0m in \u001b[0;36mxgboostTrain\u001b[0;34m(X, y, testX, xgb_final, versionSaved, params)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m#保存测试集的预测结果\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./originalDataset/exampleSubmission.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./outputs/submission%s.csv'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversionSaved\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/liwb/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/liwb/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/liwb/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/liwb/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/liwb/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'./originalDataset/exampleSubmission.csv' does not exist"
     ]
    }
   ],
   "source": [
    "xgb_final = xgb.XGBClassifier(booster='gbtree',\n",
    "                              silent=True,\n",
    "                              #n_jobs=5, #不设置的话，自动获得最大线程数\n",
    "                              #以上为general params\n",
    "                              \n",
    "                              learning_rate = 0.1,#在xgboost的package中等价于eta参数\n",
    "                              min_child_weight = 2, #控制过拟合，越大越不会过拟合\n",
    "                              \n",
    "                              max_depth=100, #控制过拟合，越小越不会过拟合\n",
    "                              max_delta_step = 1, #数据不均衡的时候可以用\n",
    "                              gamma = 0,         #模型在默认情况下，对于一个节点的划分\n",
    "                                                 #只有在其loss function 得到结果大于0的情况下才进行，\n",
    "                                                 #而gamma 给定了所需的最低loss function的值.\n",
    "                                                 #所以gamma越大越保守（conservation）\n",
    "                              subsample = 0.8,    #太大会过拟合，太小会欠拟合\n",
    "                              colsample_bytree=0.8, #\n",
    "                              reg_lambda = 100, #L2正则化的权重参数,给linear模型用的。gbtree不用\n",
    "                              reg_alpha = 0, #L1正则化的权重参数,给linear模型用的。gbtree不用\n",
    "                              scale_pos_weight=340,\n",
    "                              #以上是booster的参数\n",
    "                              \n",
    "                              \n",
    "                              random_state = 0,\n",
    "                              n_estimators = 200)#树的棵树\n",
    "\n",
    "#xgboostTrain(X, y, testX, xgb_final, versionSaved= '200depth')\n",
    "xgboostTrain(X_mx, y_mx, testX_mx, xgb_final, versionSaved= '0501_mx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 用gridsearch进行调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112405, 1136) (112405,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1}\n",
      "0.8847955576284819\n"
     ]
    }
   ],
   "source": [
    "param_test1 = {\n",
    "    #'n_estimators':[i for i in range(30,50,2)],\n",
    "    #'max_depth':[i for i in range(3,10)],\n",
    "    #'min_child_weight':[i for i in range(1,6)],\n",
    "    #'gamma':[i/10.0 for i in range(5)],\n",
    "    #'subsample':[i/10.0 for i in range(5,10)],\n",
    "    #'colsample_bytree':[i/10.0 for i in range(5,10)],\n",
    "    #'reg_lambda':[1e-5, 1e-2, 0.1, 1, 100],\n",
    "    #'max_delta_step':[i for i in range(1,10)],\n",
    "    'learning_rate':[0.001, 0.005, 0.01, 0.05, 0.1, 0.2]\n",
    "}\n",
    "xgb_model = xgb.XGBClassifier(booster='gbtree',\n",
    "                              silent=True,\n",
    "                              #n_jobs=5, #不设置的话，自动获得最大线程数\n",
    "                              #以上为general params\n",
    "                              \n",
    "                              learning_rate = 0.1,#在xgboost的package中等价于eta参数\n",
    "                              min_child_weight = 2, #控制过拟合，越大越不会过拟合\n",
    "                              \n",
    "                              max_depth=9, #控制过拟合，越小越不会过拟合\n",
    "                              max_delta_step = 1, #数据不均衡的时候可以用\n",
    "                              gamma = 0,         #模型在默认情况下，对于一个节点的划分\n",
    "                                                 #只有在其loss function 得到结果大于0的情况下才进行，\n",
    "                                                 #而gamma 给定了所需的最低loss function的值.\n",
    "                                                 #所以gamma越大越保守（conservation）\n",
    "                              subsample = 0.8,    #太大会过拟合，太小会欠拟合\n",
    "                              colsample_bytree=0.8, #\n",
    "                              reg_lambda = 100, #L2正则化的权重参数,给linear模型用的。gbtree不用\n",
    "                              reg_alpha = 0, #L1正则化的权重参数,给linear模型用的。gbtree不用\n",
    "                              scale_pos_weight=340,\n",
    "                              #以上是booster的参数\n",
    "                              \n",
    "                              \n",
    "                              random_state = 0,\n",
    "                              n_estimators = 48)#树的棵树\n",
    "\n",
    "gsearch1 = GridSearchCV(estimator=xgb_model, \n",
    "                        param_grid = param_test1, \n",
    "                        scoring = \"roc_auc\",\n",
    "                        n_jobs = 4,\n",
    "                        iid = False, \n",
    "                        cv = 5)\n",
    "\n",
    "\n",
    "gsearch1.fit(X, y)\n",
    "print(gsearch1.best_params_)\n",
    "#print(gsearch1.best_estimator_)\n",
    "print(gsearch1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_child_weight': 2, 'max_depth': 9}\n",
      "0.8760356633477311\n"
     ]
    }
   ],
   "source": [
    "#print(gsearch1.cv_results_)\n",
    "print(gsearch1.best_params_)\n",
    "#print(gsearch1.best_estimator_)\n",
    "print(gsearch1.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### stacking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import cross_validation, metrics   #Additional     scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt  \n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SklearnHelper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "        \n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        return self.clf.fit(x,y)\n",
    "    \n",
    "    def predict_proba(self, x):\n",
    "        return self.clf.predict_proba(x)\n",
    "    \n",
    "    def feature_importances(self, x, y):\n",
    "        print(self.clf.fit(x,y).feature_importances_)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random Forest parameters\n",
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 500,\n",
    "     'warm_start': True, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose': 0,\n",
    "    'class_weight':{0:1,1:340},\n",
    "    'random_state':0\n",
    "}\n",
    "\n",
    "# Extra Trees Parameters\n",
    "et_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators':500,\n",
    "    #'max_features': 0.5,\n",
    "    'max_depth': 8,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0,\n",
    "    'random_state':0\n",
    "}\n",
    "\n",
    "# AdaBoost parameters\n",
    "ada_params = {\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate' : 0.75,\n",
    "    'random_state':0\n",
    "}\n",
    "\n",
    "# Gradient Boosting parameters\n",
    "gb_params = {\n",
    "    'n_estimators': 500,\n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'subsample':0.8,\n",
    "    'verbose': 0,\n",
    "    'random_state':0\n",
    "}\n",
    "\n",
    "# Support Vector Classifier parameters \n",
    "svc_params = {\n",
    "    'kernel' : 'linear',\n",
    "    'C' : 0.025\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Create 5 objects that represent our 4 models\n",
    "SEED = 0\n",
    "rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\n",
    "et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\n",
    "ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\n",
    "gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\n",
    "svc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_oof(clf, X, y, testX, Kfold):\n",
    "    skf = StratifiedKFold(n_splits=Kfold,random_state=random_state) #k fold交叉验证\n",
    "    \n",
    "    oof_test_skf = np.empty((Kfold, testX.shape[0]))\n",
    "    oof_train = np.zeros((X.shape[0],))\n",
    "    oof_test = np.zeros((testX.shape[0],))\n",
    "    i=0\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        clf.train(X[train_index], y[train_index])\n",
    "        oof_train[test_index] = clf.predict(X[test_index])\n",
    "        \n",
    "        oof_test_skf[i,:] = clf.predict(testX)\n",
    "        i += 1\n",
    "    \n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)#测试集需要求均值\n",
    "    \n",
    "    return oof_train.reshape(-1,1), oof_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liwb/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/forest.py:304: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is complete\n"
     ]
    }
   ],
   "source": [
    "# Create our OOF train and test predictions. These base results will be used as new features\n",
    "et_oof_train, et_oof_test = get_oof(et, X, y, testX,5) # Extra Trees\n",
    "rf_oof_train, rf_oof_test = get_oof(rf,X, y, testX,5) # Random Forest\n",
    "ada_oof_train, ada_oof_test = get_oof(ada, X, y, testX,5) # AdaBoost \n",
    "gb_oof_train, gb_oof_test = get_oof(gb,X, y, testX,5) # Gradient Boost\n",
    "svc_oof_train, svc_oof_test = get_oof(svc,X, y, testX,5) # Support Vector Classifier\n",
    "\n",
    "print(\"Training is complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\n",
    "x_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112405, 5) (28101, 5)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0.9, learning_rate=0.1,\n",
       "       max_delta_step=1, max_depth=5, min_child_weight=2, missing=None,\n",
       "       n_estimators=480, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=100, scale_pos_weight=340, seed=None, silent=True,\n",
       "       subsample=0.8)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier(booster='gbtree',\n",
    "                              silent=True,\n",
    "                              #n_jobs=5, #不设置的话，自动获得最大线程数\n",
    "                              #以上为general params\n",
    "                              \n",
    "                              learning_rate = 0.1,#在xgboost的package中等价于eta参数\n",
    "                              min_child_weight = 2, #控制过拟合，越大越不会过拟合\n",
    "                              \n",
    "                              max_depth=5, #控制过拟合，越小越不会过拟合\n",
    "                              max_delta_step = 1, #数据不均衡的时候可以用\n",
    "                              gamma = 0.9,         #模型在默认情况下，对于一个节点的划分\n",
    "                                                 #只有在其loss function 得到结果大于0的情况下才进行，\n",
    "                                                 #而gamma 给定了所需的最低loss function的值.\n",
    "                                                 #所以gamma越大越保守（conservation）\n",
    "                              subsample = 0.8,    #太大会过拟合，太小会欠拟合\n",
    "                              colsample_bytree=0.8, #\n",
    "                              reg_lambda = 100, #L2正则化的权重参数,给linear模型用的。gbtree不用\n",
    "                              reg_alpha = 0, #L1正则化的权重参数,给linear模型用的。gbtree不用\n",
    "                              scale_pos_weight=340,\n",
    "                              #以上是booster的参数\n",
    "                             \n",
    "                              \n",
    "                              random_state = 0,\n",
    "                              n_estimators = 480)#树的棵树\n",
    "\n",
    "xgb_model.fit(x_train, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "res = pd.read_csv('./outputs/submission200depth_mx.csv')\n",
    "print(res.columns)\n",
    "#sum(res['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151.84400610420562"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(res['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.where(res['label'].values < 0.5, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
